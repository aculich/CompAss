{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlap in Lexical and Literary Vocabulary\n",
    "\n",
    "Comparing the vocabulary of Old Babylonian lexical texts (from Nippur) and the vocabulary of the Sumerian literary corpus as represented in [ETCSL](http://etcsl.orinst.ox.ac.uk/).\n",
    "\n",
    "## Preparation \n",
    "In order to run this notebook, parse the [DCCLT](http://oracc.org/dcclt) data with the Extended ORACC Parser (2.3) and the [ETCSL](http://etcsl.orinst.ox.ac.uk) data with the ETCSL Parser (2.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_venn import venn2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read ETCSL Data Files\n",
    "Open the file `parsed.csv`which contains all the [ETCSL](http://etcsl.orinst.ox.ac.uk) and read the data into a `Pandas`DataFrame. Each row is a word from [ETCSL](http://etcsl.orinst.ox.ac.uk/) in lemmatized format, according to [ePSD2](http://build-oracc.museum.upenn.edu/epsd2) standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = \"../2_4_Data_Acquisition_ETCSL/Output/alltexts.csv\"\n",
    "etcsl = pd.read_csv(file).fillna(\"\")\n",
    "etcsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmas\n",
    "Create a lemmas column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl[\"lemma\"] = etcsl.apply(lambda r: (r[\"cf\"] + '[' + r[\"gw\"] + ']' + r[\"pos\"]) \n",
    "                            if r[\"gw\"] != '' else r['form'] + '[NA]NA', axis=1)\n",
    "etcsl['lemma'] = [lemma if not lemma == '[NA]NA' else '' for lemma in etcsl['lemma'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line by Line\n",
    "Rearrange the dataframe to a line-by-line representation of the [ETCSL](http://etcsl.orinst.ox.ac.uk) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl = etcsl.groupby([etcsl['id_text'], etcsl['line_ref'], etcsl['text_name']]).agg({\n",
    "        'lemma': ' '.join,\n",
    "        'extent': ''.join\n",
    "    }).reset_index()\n",
    "etcsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Lexical Data\n",
    "\n",
    "Essentially following the same process as in the preceding cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = \"../2_3_Data_Acquisition_ORACC/output/parsed.csv\"\n",
    "lexical = pd.read_csv(file).fillna(\"\")\n",
    "lexical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Lexical Compositions\n",
    "Select the following compositions: \n",
    "* Ura 1 dcclt/Q000039\n",
    "* Ura 2 dcclt/Q000040\n",
    "* Ura 3 dcclt/Q000001\n",
    "* Ura 4 dcclt/Q000041\n",
    "* Ura 5 dcclt/Q000042\n",
    "* Ura 6 dcclt/Q000043\n",
    "* Lu₂-Azlag₂ B/C Q000302 \n",
    "* Ugumu dcclt/Q00268\n",
    "* Diri dcclt/Q000057\n",
    "* Nigga dcclt/Q000052\n",
    "* Izi dcclt/Q000050\n",
    "* Kagal dcclt/Q000048\n",
    "* Lu dcclt/Q000047\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep = [\"dcclt/Q000039\", \n",
    "    \"dcclt/Q000040\",\n",
    "    \"dcclt/Q000001\",\n",
    "    \"dcclt/Q000041\",\n",
    "    \"dcclt/Q000042\",\n",
    "    \"dcclt/Q000043\",\n",
    "    \"dcclt/Q000302 \",\n",
    "    \"dcclt/Q00268\",\n",
    "    \"dcclt/Q000057\",\n",
    "    \"dcclt/Q000052\",\n",
    "    \"dcclt/Q000050\",\n",
    "    \"dcclt/Q000048\",\n",
    "    \"dcclt/Q000047\"]\n",
    "lexical = lexical.loc[lexical[\"id_text\"].isin(keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Approximation\n",
    "Extract the vocabulary of ETCSL and the lexical texts. Count each word separately. Thus, in the lexical item `gud an-na` we count the words `gud` (ox) and `an` (heaven) separately to see if those words appear in the literary corpus as well. This approach, therefore, does not take account of attestations of `gud an-na` in both corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl_words = ' '.join(list(etcsl[\"lemma\"]))\n",
    "lexical_words = \" \".join(list(lexical[\"lemma\"]))\n",
    "etcsl_words_s = set(etcsl_words.split())\n",
    "lexical_words_s = set(lexical_words.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "lit_legend = \"literary (\" + str(len(etcsl_words_s)) + ')'\n",
    "lex_legend = \"lexical (\" + str(len(lexical_words_s)) + \")\"\n",
    "venn2([etcsl_words_s, lexical_words_s], (lit_legend, lex_legend))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mark Multiple Word Expressions\n",
    "\n",
    "Iterate through the list of lexical vocabulary (beginning with the longest items) and check to see if the item contains multiple lemmatizations (contains spaces). If so, the item is an `expression` and the spaces are replaced by `*`. This will ensure that the expressions are dealt with as single units, for instance `gud an-na` (Bull of Heaven), lemmatized as `gud[bull]n an[heaven]n` becomes the unit `gud[bull]n*an[heaven]n`.\n",
    "\n",
    "The method `.str.replace()` in `Pandas` uses regular expressions - a setting that cannot be switched off. It is necessary, therefore, to indicate that certain characters are to be read literally and not as regex special characters. This is true, in particular, for the characters `[` and `]` that surround the Guide Word (as in `gud[bull]n`). These characters, therefore, are preceded by the `escape character` `\\` in the search/replace function.\n",
    "\n",
    "The constituent words (lemmatizations) of all expressions are now connected by `*` both in the `etcsl` dataframe and in the `lex_vocab` list.\n",
    "\n",
    "The resulting Venn diagram shows how many lexical *entries* are attested in the literary corpus. Of course, many (but not all) individual words (such as `gud` and `an`) are indeed entries somewhere in the lexical corpus and are therefore also registered in the overlap section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lex_vocab = list(set(lexical[\"lemma\"]))\n",
    "lex_vocab = [item.replace(\" \", \"*\") for item in lex_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lex_vocab.sort(key=len, reverse=True)\n",
    "for item in lex_vocab:\n",
    "    if \"*\" in item:\n",
    "        findwhat = item.replace(\"[\", \"\\[\")\n",
    "        findwhat = findwhat.replace(\"]\", \"\\]\")\n",
    "        findwhat = findwhat.replace(\"*\", \" \")\n",
    "        etcsl[\"lemma\"] = etcsl[\"lemma\"].str.replace(findwhat, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl[etcsl[\"text\"].str.contains(\"*\", regex=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "etcsl_words2 = ' '.join(list(etcsl[\"text\"]))\n",
    "etcsl_words_s2 = set(etcsl_words2.split())\n",
    "lexical_words_s2 = set(lex_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "lit_legend = \"literary (\" + str(len(etcsl_words_s2)) + ')'\n",
    "lex_legend = \"lexical (\" + str(len(lexical_words_s2)) + \")\"\n",
    "venn2([etcsl_words_s2, lexical_words_s2], (lit_legend, lex_legend))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add them Up\n",
    "By creating the union of the two sets (the set with individual words and the set with the lexical entries only) we get the most complete comparison of the two corpora. Here `gud[oxen]N*an[heaven]N`, `gud[oxen]N` and `an[heaven]N` are all counted as words, whether or not `gud` and `an` actually appear as such in the lexical corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl_words_s3 = etcsl_words_s | etcsl_words_s2\n",
    "lexical_words_s3 = lexical_words_s | lexical_words_s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "lit_legend = \"literary (\" + str(len(etcsl_words_s3)) + ')'\n",
    "lex_legend = \"lexical (\" + str(len(lexical_words_s3)) + \")\"\n",
    "venn2([etcsl_words_s3, lexical_words_s3], (lit_legend, lex_legend))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(etcsl_words_s), len(etcsl_words_s2), len(etcsl_words_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Corpus\n",
    "The etcsl dataframe is organized by line. The `aggregate` function assembles the lines that belong to a single composition. The resulting dataframe has 394 entries, one for each composition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = etcsl.groupby([etcsl[\"etcsl_no\"], etcsl[\"text_name\"]]).aggregate({\"text\": \" \".join}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Term Matrix\n",
    "The corpus is transformed into a Document Term Matrix (or DTM) in which each word (or expression) is a column and each row a Sumerian composition. The lexical vocabulary is used as a filter (only words that appear in the lexical texts are allowed in the DTM).\n",
    "\n",
    "First sort the lexical vocabulary alphabetically.\n",
    "\n",
    "Use CountVectorizer (from Sklearn) to initiate the DTM. The `token_pattern` indicates what a word looks like and what signals the end of a word. In modern corpora this can be fairly complex (including spaces, commas, full stops, etc.) but in the current corpus a space will always separate one token from the next. The regex `r'[^ ]+'` means \"any sequence of characters, except space\".\n",
    "\n",
    "Transform the DTM to a dataframe.\n",
    "\n",
    "NB a DTM is not necessary for the raw comparison/Venn diagram - only for further inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer='word', token_pattern=r'[^ ]+', binary = False, vocabulary=lex_vocab)\n",
    "etcsl_dtm = cv.fit_transform(corpus['text'])\n",
    "#etcsl_df = pd.DataFrame(etcsl_dtm.toarray(), columns= cv.get_feature_names(), index=corpus[\"etcsl_no\"])\n",
    "etcsl_df = pd.DataFrame(etcsl_dtm.toarray(), columns= lex_vocab, index=corpus[\"etcsl_no\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words in Lexical Texts not in ETCSL\n",
    "If a word or expression in the lexical corpus is never used in the literary texts from [ETCSL](http://etcsl.orinst.ox.ac.uk/) the sum of its column will be `0`.\n",
    "\n",
    "Give the number of columns (the number of unique words and expressions in the lexical texts), the number of words/expressions never used in the ETCSL corpus and the relation between those two numbers in percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lex_not_in_etcsl = etcsl_df.loc[:, etcsl_df.sum()==0]\n",
    "len(etcsl_df.columns), len(lex_not_in_etcsl.columns), str(len(lex_not_in_etcsl.columns)/len(etcsl_df.columns)*100) + \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplify\n",
    "The above may be an overly complex way of doing it.\n",
    "Alternative: make a full dtm of etcsl (without a vocabulary constraint); make the etcsl vocabulary and lexical vocabulary into sets that can be subtracted from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer='word', token_pattern=r'[^ ]+', binary = False)\n",
    "etcsl2_dtm = cv.fit_transform(corpus['text'])\n",
    "etcsl2_df = pd.DataFrame(etcsl2_dtm.toarray(), columns= cv.get_feature_names(), index=corpus[\"etcsl_no\"])\n",
    "etcsl_vocab_s = set(etcsl2_df.columns)\n",
    "lex_vocab_s = set(lex_vocab)\n",
    "diff_e_l = list(etcsl_vocab_s - lex_vocab_s)\n",
    "diff_l_e = list(lex_vocab_s - etcsl_vocab_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"number of words/expressions in ETCSL \" + str(len(etcsl_vocab_s)))\n",
    "print(\"number of words/expressions in lexical texts \" + str(len(lex_vocab_s)))\n",
    "print(\"number of words/expressions in ETCSL not in lexical \" + str(len(diff_e_l)))\n",
    "print(\"number of words/expressions in lexical not in ETCSL \" + str(len(diff_l_e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "venn2([etcsl_vocab_s, lex_vocab_s], (\"literary\", \"lexical\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare Words Shared by Lex and Lit\n",
    "Which words appear in Lex and in Lit but appear only once in Lit? In which composition do we find such words; which words are those?\n",
    "\n",
    "First create a dataframe (`rare`) that only has the columns that add up to `1` (word or expression appears only once in the corpus). The row totals of this dataframe indicate per composition (= row) how many such rare words they contain. These row totals are added as a separate column. The composition naes are extracted from the `corpus` dataframe created above. Finally the dataframe is sorted by the row totals.\n",
    "\n",
    "The dataframe `rare` includes columns for each of the words that appear only once. We are showing only the columns that identify the composition and the row totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rare =etcsl_df.loc[:, etcsl_df.sum()==1].reset_index()\n",
    "rare[\"no. of unique lexical correspondences\"] = rare.sum(axis=1)\n",
    "rare[\"text_name\"] = corpus[\"text_name\"]\n",
    "rare = rare.sort_values('no. of unique lexical correspondences', ascending = False)\n",
    "rare.loc[:,[\"etcsl_no\", \"no. of unique lexical correspondences\", \"text_name\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which Words?\n",
    "Which are the rare words that define this list of compositions? We first extract the full list of words from the column names of the daraframe `rare`. The variable `words` is a Numpy array that contains strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = rare.columns.values\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The rare words in the top-ten\n",
    "The first ten compositions in our list are the ones that have the most rare words shared with lexical texts. Each row, representing a composition, has columns that represent individual words. We create a `mask` (a sequence of boolean values `True` or `False`) that indicate whether or not the value in the column is 1. If the boolean is `True` the word is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    indexes = rare.iloc[i] == 1\n",
    "    print(rare.iloc[i,-1]), print(words[indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexical[\"text\"] = lexical[\"text\"].str.replace(\" \", \"*\")\n",
    "lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexical_corpus = lexical.groupby([lexical[\"id_text\"], \n",
    "                                  lexical[\"text_name\"]]).aggregate({\"text\": \" \".join}).reset_index()\n",
    "lexical_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
