{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlap in Lexical and Literary Vocabulary\n",
    "\n",
    "Comparing the vocabulary of Old Babylonian lexical texts (from Nippur) and the vocabulary of the Sumerian literary corpus as represented in [ETCSL](http://etcsl.orinst.ox.ac.uk/).\n",
    "\n",
    "## Preparation \n",
    "In order to run this notebook, parse the [DCCLT](http://oracc.org/dcclt) data with the Extended ORACC Parser (2.3.3) and the [ETCSL](http://etcsl.orinst.ox.ac.uk) data with the ETCSL Parser (2.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read ETCSL Data Files\n",
    "Open the file `alltexts.csv` which contains all the [ETCSL](http://etcsl.orinst.ox.ac.uk) and read the data into a `Pandas`DataFrame. Each row is a word from [ETCSL](http://etcsl.orinst.ox.ac.uk/) in lemmatized format, according to [ePSD2](http://build-oracc.museum.upenn.edu/epsd2) standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = \"../2_4_Data_Acquisition_ETCSL/Output/alltexts.csv\"\n",
    "etcsl = pd.read_csv(file, keep_default_na=False)\n",
    "etcsl = etcsl.loc[etcsl[\"lang\"].str.contains(\"sux\")]  # throw out non-Sumerian words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmas\n",
    "Create a lemmas column and lowercase all lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl[\"lemma\"] = etcsl.apply(lambda r: (r[\"cf\"] + '[' + r[\"gw\"] + ']' + r[\"pos\"]) \n",
    "                            if r[\"cf\"] != '' else r['form'] + '[NA]NA', axis=1)\n",
    "etcsl['lemma'] = [lemma if not lemma == '[NA]NA' else '' for lemma in etcsl['lemma'] ] \n",
    "# kick out empty forms\n",
    "etcsl[\"lemma\"] = etcsl[\"lemma\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Lexical Data\n",
    "\n",
    "Essentially following the same process as in the preceding cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = \"../2_3_Data_Acquisition_ORACC/output/parsed.csv\"\n",
    "lexical = pd.read_csv(file, keep_default_na=False)\n",
    "lexical = lexical.loc[lexical[\"lang\"].str.contains(\"sux\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexical[\"lemma\"] = lexical.apply(lambda r: (r[\"cf\"] + '[' + r[\"gw\"] + ']' + r[\"pos\"]) \n",
    "                            if r[\"cf\"] != '' else r['form'] + '[NA]NA', axis=1)\n",
    "lexical['lemma'] = [lemma if not lemma == '[NA]NA' else '' for lemma in lexical['lemma'] ] \n",
    "# kick out empty forms\n",
    "lexical[\"lemma\"] = lexical[\"lemma\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Lexical Compositions\n",
    "Select the following compositions: \n",
    "* Ura 1 dcclt/Q000039\n",
    "* Ura 2 dcclt/Q000040\n",
    "* Ura 3 dcclt/Q000001\n",
    "* Ura 4 dcclt/Q000041\n",
    "* Ura 5 dcclt/Q000042\n",
    "* Ura 6 dcclt/Q000043\n",
    "* Lu₂-Azlag₂ B/C Q000302 \n",
    "* Ugumu dcclt/Q000268\n",
    "* Diri dcclt/Q000057\n",
    "* Nigga dcclt/Q000052\n",
    "* Izi dcclt/Q000050\n",
    "* Kagal dcclt/Q000048\n",
    "* Lu dcclt/Q000047\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep = [\"dcclt/Q000039\", \n",
    "    \"dcclt/Q000040\",\n",
    "    \"dcclt/Q000001\",\n",
    "    \"dcclt/Q000041\",\n",
    "    \"dcclt/Q000042\",\n",
    "    \"dcclt/Q000043\",\n",
    "    \"dcclt/Q000302 \",\n",
    "    \"dcclt/Q000268\",\n",
    "    \"dcclt/Q000057\",\n",
    "    \"dcclt/Q000052\",\n",
    "    \"dcclt/Q000050\",\n",
    "    \"dcclt/Q000048\",\n",
    "    \"dcclt/Q000047\"]\n",
    "lexical = lexical.loc[lexical[\"id_text\"].isin(keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexical = lexical[~lexical[\"field\"].isin([\"sg\", \"pr\"])] # remove lemams that derive from the fields \"sign\" \n",
    "# or \"pronunciation\" in sign lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Approximation\n",
    "Extract the vocabulary of ETCSL and the lexical texts. Count each word separately. Thus, in the lexical item `gud an-na` we count the words `gud` (ox) and `an` (heaven) separately to see if those words appear in the literary corpus as well. This approach, therefore, does not take account of attestations of `gud an-na` in both corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl_words_s = set(list(etcsl[\"lemma\"]))\n",
    "lexical_words_s = set(list(lexical[\"lemma\"]))\n",
    "etcsl_words_s = {lemma for lemma in etcsl_words_s if not '[na]na' in lemma}\n",
    "lexical_words_s = {lemma for lemma in lexical_words_s if not '[na]na' in lemma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_venn(lit_vocab, lex_vocab):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    lit_abs = len(lit_vocab)\n",
    "    lex_abs = len(lex_vocab)\n",
    "    inter_abs = len(lit_vocab.intersection(lex_vocab))\n",
    "    lit_per = \"{:.0%}\".format(inter_abs/lit_abs)\n",
    "    lex_per = \"{:.0%}\".format(inter_abs/lex_abs)\n",
    "    lit_legend = \"literary (\" + str(lit_abs) + ') ' + lit_per + \" overlap\"\n",
    "    lex_legend = \"lexical (\" + str(lex_abs) + ') ' + lex_per + \" overlap\"\n",
    "    c = venn2([lit_vocab, lex_vocab], (lit_legend, lex_legend))\n",
    "    c.get_patch_by_id('10').set_color(\"#fdb515\")\n",
    "    c.get_patch_by_id('01').set_color(\"#003262\")\n",
    "    c.get_patch_by_id('11').set_color(\"#bc9b6a\")\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_venn(etcsl_words_s, lexical_words_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mark Multiple Word Expressions\n",
    "\n",
    "Instead of looking at individual words (or lexemes), we may also look at lexical *entries* and their appearance (or not) in literary texts. The list of domestic animals, for instance, includes the entry `udu dijir-e gu₇-a`('sheep eaten by a god'), lemmatized as `udu[sheep]n diŋir[god]n gu[eat]v/t`. Unsurprisingly, all these lemmas appear in the literary corpus, and thus in our previous analysis this item results in three hits. But does the expression as a whole ever appear in the literary corpus? \n",
    "\n",
    "In order to perform the comparison on the lexical entry level we first need to represent our data (lexical and literary) as lines (rather than individual words).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line by Line\n",
    "In order to reconstruct lines we the Pandas function `groupby()` with the fields `id_text` and `id_line`. The `agg()` function indicates which fields (beside `id_text` and `id_line`) are kept and how they are treated (see also 2.3.???)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexical = lexical.groupby([lexical['id_text'], lexical['id_line']]).agg({\n",
    "        'lemma': ' '.join,\n",
    "        'extent': ''.join\n",
    "    }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "etcsl = etcsl.groupby([etcsl['id_text'], etcsl['id_line'], etcsl['text_name']]).agg({\n",
    "        'lemma': ' '.join,\n",
    "        'extent': ''.join\n",
    "    }).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract lexical entries \n",
    "Each item in the resulting list now consists of the lemmas of an entire lexical *entry* (e.g. `udu[sheep]n diŋir[god]n gu[eat]v/t`). Replace all spaces by asterisks, so that the whole string can be treated as a unity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lex_vocab = list(set(lexical[\"lemma\"]))\n",
    "lex_vocab = [item.replace(\" \", \"*\") for item in lex_vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark lexical entries in literary texts\n",
    "Iterate through the list of lexical vocabulary (beginning with the longest items) and check to see if the item contains multiple lemmatizations (contains asterisks). If so, the `str.replace()` function of Pandas is used to find identical sequences of lemmas in a line in the `etcsl` DataFrame and to equally replace spaces by asterisks.\n",
    "\n",
    "In order to do so we create a `findwhat` variable that is derived from the lexical item. First the asterisks are  replaced again by spaces (we are looking for, say, `udu[sheep]n diŋir[god]n gu[eat]v/t`). Second, all characters that are interpreted as [regular expression](https://www.regular-expressions.info/) special characters (such as square brackets) need to be escaped (prefixed by a backslash), so that they are interpreted as literals by the `str.replace()` function. \n",
    "\n",
    "The `str.replace()` function takes a considerable amount of time. It has to check each line in [ETCSL](http://etcsl.orinst.ox.ac.uk/) (almost 36.500 lines) against each lexical entry (almost 6,000 entries). The progress bar follows the progress through the list of lexical entries.\n",
    "\n",
    "The `lemma` column of the `etcsl` dataframe will now represent the [ETCSL](http://etcsl.orinst.ox.ac.uk/) data in a line-by-line presentation of lemmatizations, with asterisks connecting lemmas if a corresponding sequence of lemmas exists as a lexical entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lex_vocab.sort(key=len, reverse=True)\n",
    "for item in tqdm.tqdm(lex_vocab):\n",
    "    if \"*\" in item:\n",
    "        findwhat = item.replace(\"*\", \" \")\n",
    "        findwhat = re.escape(findwhat)\n",
    "        etcsl[\"lemma\"] = etcsl[\"lemma\"].str.replace(findwhat, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "etcsl_words2 = ' '.join(list(etcsl[\"lemma\"]))\n",
    "etcsl_words_s2 = set(etcsl_words2.split())\n",
    "lexical_words_s2 = set(list(lexical[\"lemma\"]))\n",
    "etcsl_words_s2 = {lemma for lemma in etcsl_words_s2 if not '[na]na' in lemma}\n",
    "lexical_words_s2 = {lemma for lemma in lexical_words_s2 if not '[na]na' in lemma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_venn(etcsl_words_s2, lexical_words_s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add them Up\n",
    "By creating the union of the two sets (the set with individual words and the set with the lexical entries only) we get the most complete comparison of the two corpora. Here `gud[oxen]N*an[heaven]N`, `gud[oxen]N` and `an[heaven]N` are all counted as words, whether or not `gud` and `an` actually appear as such in the lexical corpus.\n",
    "\n",
    "It turns out that this addition adds only twenty words to the literary vocabulary. These are words that appear in lexical expressions (connected by \\*), but never appear on their own outside such expressions. The lexical corpus has more of those lexemes: more than 700; about half of those also appear in the literary corpus, increasing the overlap by about 370."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl_words_s3 = etcsl_words_s | etcsl_words_s2\n",
    "lexical_words_s3 = lexical_words_s | lexical_words_s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_venn(etcsl_words_s3, lexical_words_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Corpus\n",
    "The etcsl dataframe is organized by line. The `aggregate` function assembles the lines that belong to a single composition. The resulting dataframe has 394 entries, one for each composition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = etcsl.groupby([etcsl[\"etcsl_no\"], etcsl[\"text_name\"]]).aggregate({\"text\": \" \".join}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Term Matrix\n",
    "The corpus is transformed into a Document Term Matrix (or DTM) in which each word (or expression) is a column and each row a Sumerian composition. The lexical vocabulary is used as a filter (only words that appear in the lexical texts are allowed in the DTM).\n",
    "\n",
    "First sort the lexical vocabulary alphabetically.\n",
    "\n",
    "Use CountVectorizer (from Sklearn) to initiate the DTM. The `token_pattern` indicates what a word looks like and what signals the end of a word. In modern corpora this can be fairly complex (including spaces, commas, full stops, etc.) but in the current corpus a space will always separate one token from the next. The regex `r'[^ ]+'` means \"any sequence of characters, except space\".\n",
    "\n",
    "Transform the DTM to a dataframe.\n",
    "\n",
    "NB a DTM is not necessary for the raw comparison/Venn diagram - only for further inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer='word', token_pattern=r'[^ ]+', binary = False, vocabulary=lex_vocab)\n",
    "etcsl_dtm = cv.fit_transform(corpus['text'])\n",
    "#etcsl_df = pd.DataFrame(etcsl_dtm.toarray(), columns= cv.get_feature_names(), index=corpus[\"etcsl_no\"])\n",
    "etcsl_df = pd.DataFrame(etcsl_dtm.toarray(), columns= lex_vocab, index=corpus[\"etcsl_no\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words in Lexical Texts not in ETCSL\n",
    "If a word or expression in the lexical corpus is never used in the literary texts from [ETCSL](http://etcsl.orinst.ox.ac.uk/) the sum of its column will be `0`.\n",
    "\n",
    "Give the number of columns (the number of unique words and expressions in the lexical texts), the number of words/expressions never used in the ETCSL corpus and the relation between those two numbers in percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lex_not_in_etcsl = etcsl_df.loc[:, etcsl_df.sum()==0]\n",
    "len(etcsl_df.columns), len(lex_not_in_etcsl.columns), str(len(lex_not_in_etcsl.columns)/len(etcsl_df.columns)*100) + \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplify\n",
    "The above may be an overly complex way of doing it.\n",
    "Alternative: make a full dtm of etcsl (without a vocabulary constraint); make the etcsl vocabulary and lexical vocabulary into sets that can be subtracted from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer='word', token_pattern=r'[^ ]+', binary = False)\n",
    "etcsl2_dtm = cv.fit_transform(corpus['text'])\n",
    "etcsl2_df = pd.DataFrame(etcsl2_dtm.toarray(), columns= cv.get_feature_names(), index=corpus[\"etcsl_no\"])\n",
    "etcsl_vocab_s = set(etcsl2_df.columns)\n",
    "lex_vocab_s = set(lex_vocab)\n",
    "diff_e_l = list(etcsl_vocab_s - lex_vocab_s)\n",
    "diff_l_e = list(lex_vocab_s - etcsl_vocab_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"number of words/expressions in ETCSL \" + str(len(etcsl_vocab_s)))\n",
    "print(\"number of words/expressions in lexical texts \" + str(len(lex_vocab_s)))\n",
    "print(\"number of words/expressions in ETCSL not in lexical \" + str(len(diff_e_l)))\n",
    "print(\"number of words/expressions in lexical not in ETCSL \" + str(len(diff_l_e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "venn2([etcsl_vocab_s, lex_vocab_s], (\"literary\", \"lexical\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare Words Shared by Lex and Lit\n",
    "Which words appear in Lex and in Lit but appear only once in Lit? In which composition do we find such words; which words are those?\n",
    "\n",
    "First create a dataframe (`rare`) that only has the columns that add up to `1` (word or expression appears only once in the corpus). The row totals of this dataframe indicate per composition (= row) how many such rare words they contain. These row totals are added as a separate column. The composition naes are extracted from the `corpus` dataframe created above. Finally the dataframe is sorted by the row totals.\n",
    "\n",
    "The dataframe `rare` includes columns for each of the words that appear only once. We are showing only the columns that identify the composition and the row totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rare =etcsl_df.loc[:, etcsl_df.sum()==1].reset_index()\n",
    "rare[\"no. of unique lexical correspondences\"] = rare.sum(axis=1)\n",
    "rare[\"text_name\"] = corpus[\"text_name\"]\n",
    "rare = rare.sort_values('no. of unique lexical correspondences', ascending = False)\n",
    "rare.loc[:,[\"etcsl_no\", \"no. of unique lexical correspondences\", \"text_name\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which Words?\n",
    "Which are the rare words that define this list of compositions? We first extract the full list of words from the column names of the daraframe `rare`. The variable `words` is a Numpy array that contains strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = rare.columns.values\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The rare words in the top-ten\n",
    "The first ten compositions in our list are the ones that have the most rare words shared with lexical texts. Each row, representing a composition, has columns that represent individual words. We create a `mask` (a sequence of boolean values `True` or `False`) that indicate whether or not the value in the column is 1. If the boolean is `True` the word is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    indexes = rare.iloc[i] == 1\n",
    "    print(rare.iloc[i,-1]), print(words[indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexical[\"text\"] = lexical[\"text\"].str.replace(\" \", \"*\")\n",
    "lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexical_corpus = lexical.groupby([lexical[\"id_text\"], \n",
    "                                  lexical[\"text_name\"]]).aggregate({\"text\": \" \".join}).reset_index()\n",
    "lexical_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
